{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In this notebook, we will try to use convolution-recurrent nural networks to solve sentiment classification problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras.callbacks import TensorBoard\n",
    "from time import time\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Input,RNN,Embedding,LSTM,Conv1D,GlobalMaxPooling1D,Flatten,MaxPooling1D,GRU,SpatialDropout1D,Bidirectional, merge\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import to_categorical\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adam, Adadelta\n",
    "from keras.constraints import maxnorm\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./data/train.tsv\", sep = \"\\t\")\n",
    "test = pd.read_csv(\"./data/test.tsv\", sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train.Phrase\n",
    "test_text = test.Phrase\n",
    "total_reviews = list(train_text) + list(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = Tokenizer(lower=True, filters='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk.fit_on_texts(total_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The max length of the reviews is 56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max([len(s.split()) for s in total_reviews])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized = tk.texts_to_sequences(train_text)\n",
    "test_tokenized = tk.texts_to_sequences(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train.Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The vocabulary size is 19479"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tk.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Padding each review to the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(train_tokenized, maxlen=max_length, padding='post')\n",
    "X_test = pad_sequences(test_tokenized, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.zeros([len(y_train), 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y_train)):\n",
    "    y[i,y_train[i]-1] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Embedding layer requires the specification of the vocabulary size **(vocab_size)**, the size of the real-valued vector space **EMBEDDING_DIM = 100**, and the maximum length of input documents **max_length** ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Start building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildModel():\n",
    "    main_input = Input(shape=(max_length, ), dtype='int32', name='main_input')\n",
    "    embedding = Embedding(vocab_size, EMBEDDING_DIM, input_length = max_length, name='embedding')(main_input)\n",
    "    embedding = Dropout(0.50)(embedding)\n",
    "    conv1 = Conv1D(200, 4, border_mode = 'valid', subsample_length=1, activation='relu', name='conv1')(embedding)\n",
    "    maxconv1 = MaxPooling1D(pool_size=2, name='maxconv1')(conv1)\n",
    "    conv2 = Conv1D(200, 5, border_mode = 'valid', subsample_length=1, activation='relu', name='conv2')(embedding)\n",
    "    maxconv2 = MaxPooling1D(pool_size=2, name='maxconv2')(conv2)\n",
    "    x = merge([maxconv1, maxconv2], mode='concat')\n",
    "    x = Dropout(0.15)(x)\n",
    "    x = LSTM(100)(x)\n",
    "    x = Dense(400, activation='relu', init='he_normal', W_constraint = maxnorm(3), b_constraint=maxnorm(3), name='mlp')(x)\n",
    "    x = Dropout(0.10, name='drop')(x)\n",
    "    output = Dense(5, init='he_normal', activation='softmax', name='output')(x)\n",
    "    model = Model(input=main_input, output=output)\n",
    "    model.compile(loss={'output':'categorical_crossentropy'}, optimizer=Adadelta(lr=0.95, epsilon=1e-06), metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/s2191989/anaconda3/envs/Zhao/lib/python3.5/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(200, 4, name=\"conv1\", padding=\"valid\", activation=\"relu\", strides=1)`\n",
      "  \"\"\"\n",
      "/data/s2191989/anaconda3/envs/Zhao/lib/python3.5/site-packages/ipykernel_launcher.py:7: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(200, 5, name=\"conv2\", padding=\"valid\", activation=\"relu\", strides=1)`\n",
      "  import sys\n",
      "/data/s2191989/anaconda3/envs/Zhao/lib/python3.5/site-packages/ipykernel_launcher.py:9: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  if __name__ == '__main__':\n",
      "/data/s2191989/anaconda3/envs/Zhao/lib/python3.5/site-packages/keras/legacy/layers.py:458: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "/data/s2191989/anaconda3/envs/Zhao/lib/python3.5/site-packages/ipykernel_launcher.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(400, name=\"mlp\", kernel_constraint=<keras.con..., activation=\"relu\", kernel_initializer=\"he_normal\", bias_constraint=<keras.con...)`\n",
      "  if sys.path[0] == '':\n",
      "/data/s2191989/anaconda3/envs/Zhao/lib/python3.5/site-packages/ipykernel_launcher.py:14: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(5, name=\"output\", activation=\"softmax\", kernel_initializer=\"he_normal\")`\n",
      "  \n",
      "/data/s2191989/anaconda3/envs/Zhao/lib/python3.5/site-packages/ipykernel_launcher.py:15: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"ou..., inputs=Tensor(\"ma...)`\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "model = BuildModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main_input (InputLayer)         (None, 56)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 56, 100)      1947900     main_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 56, 100)      0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv1D)                  (None, 53, 200)      80200       dropout_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2 (Conv1D)                  (None, 52, 200)      100200      dropout_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "maxconv1 (MaxPooling1D)         (None, 26, 200)      0           conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "maxconv2 (MaxPooling1D)         (None, 26, 200)      0           conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "merge_8 (Merge)                 (None, 26, 400)      0           maxconv1[0][0]                   \n",
      "                                                                 maxconv2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 26, 400)      0           merge_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   (None, 100)          200400      dropout_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "mlp (Dense)                     (None, 400)          40400       lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "drop (Dropout)                  (None, 400)          0           mlp[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 5)            2005        drop[0][0]                       \n",
      "==================================================================================================\n",
      "Total params: 2,371,105\n",
      "Trainable params: 2,371,105\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 124848 samples, validate on 31212 samples\n",
      "Epoch 1/10\n",
      "124848/124848 [==============================] - 169s 1ms/step - loss: 1.0247 - acc: 0.5928 - val_loss: 1.0389 - val_acc: 0.5799\n",
      "Epoch 2/10\n",
      "124848/124848 [==============================] - 172s 1ms/step - loss: 0.8837 - acc: 0.6448 - val_loss: 1.0160 - val_acc: 0.5917\n",
      "Epoch 3/10\n",
      "124848/124848 [==============================] - 173s 1ms/step - loss: 0.8311 - acc: 0.6666 - val_loss: 1.0372 - val_acc: 0.5943\n",
      "Epoch 4/10\n",
      "124848/124848 [==============================] - 172s 1ms/step - loss: 0.8049 - acc: 0.6781 - val_loss: 1.0206 - val_acc: 0.5936\n",
      "Epoch 5/10\n",
      "124848/124848 [==============================] - 172s 1ms/step - loss: 0.7841 - acc: 0.6868 - val_loss: 1.0177 - val_acc: 0.5864\n",
      "Epoch 6/10\n",
      "124848/124848 [==============================] - 173s 1ms/step - loss: 0.7711 - acc: 0.6933 - val_loss: 1.0252 - val_acc: 0.5917\n",
      "Epoch 7/10\n",
      "124848/124848 [==============================] - 174s 1ms/step - loss: 0.7596 - acc: 0.7001 - val_loss: 1.0524 - val_acc: 0.5933\n",
      "Epoch 8/10\n",
      "124848/124848 [==============================] - 168s 1ms/step - loss: 0.7475 - acc: 0.7034 - val_loss: 1.0160 - val_acc: 0.5881\n",
      "Epoch 9/10\n",
      "124848/124848 [==============================] - 167s 1ms/step - loss: 0.7393 - acc: 0.7082 - val_loss: 1.0149 - val_acc: 0.5984\n",
      "Epoch 10/10\n",
      "124848/124848 [==============================] - 169s 1ms/step - loss: 0.7310 - acc: 0.7144 - val_loss: 1.0403 - val_acc: 0.5839\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1803508cf8>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y, validation_split=0.2, epochs = 10, batch_size=50, verbose=1, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"cnn_rnn_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildCNNModel():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, EMBEDDING_DIM, input_length = max_length, name = 'embedding'))\n",
    "    model.add(Dropout(0.5, name = 'dropout1'))\n",
    "    model.add(Conv1D(200, 4, border_mode = 'valid', subsample_length=1, activation='relu', name = 'conv1'))\n",
    "    model.add(MaxPooling1D(pool_size=2, name = 'conv1_max'))\n",
    "    model.add(Dropout(0.15, name = 'dropout2'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(5, activation='softmax', name = 'dense'))\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.001),metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildRNNModel():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, EMBEDDING_DIM, input_length = max_length, name = 'embedding'))\n",
    "    model.add(Dropout(0.5, name = 'dropout1'))\n",
    "    model.add(LSTM(64, name = 'lstm'))\n",
    "    model.add(Dropout(0.15, name = 'dropout2'))\n",
    "    model.add(Dense(5, activation='softmax', name = 'dense'))\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.001),metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/s2191989/anaconda3/envs/Zhao/lib/python3.5/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(200, 4, name=\"conv1\", padding=\"valid\", activation=\"relu\", strides=1)`\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "CNN_model = BuildCNNModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_model = BuildRNNModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 124848 samples, validate on 31212 samples\n",
      "Epoch 1/10\n",
      "124848/124848 [==============================] - 18s 141us/step - loss: 0.8033 - acc: 0.6775 - val_loss: 0.9934 - val_acc: 0.6062\n",
      "Epoch 2/10\n",
      "124848/124848 [==============================] - 18s 144us/step - loss: 0.7334 - acc: 0.7061 - val_loss: 1.0184 - val_acc: 0.5970\n",
      "Epoch 3/10\n",
      "124848/124848 [==============================] - 19s 150us/step - loss: 0.6851 - acc: 0.7247 - val_loss: 1.0321 - val_acc: 0.5994\n",
      "Epoch 4/10\n",
      "124848/124848 [==============================] - 20s 163us/step - loss: 0.6466 - acc: 0.7412 - val_loss: 1.0632 - val_acc: 0.5941\n",
      "Epoch 5/10\n",
      "124848/124848 [==============================] - 19s 156us/step - loss: 0.6137 - acc: 0.7558 - val_loss: 1.0779 - val_acc: 0.5933\n",
      "Epoch 6/10\n",
      "124848/124848 [==============================] - 19s 154us/step - loss: 0.5893 - acc: 0.7647 - val_loss: 1.0922 - val_acc: 0.5917\n",
      "Epoch 7/10\n",
      "124848/124848 [==============================] - 20s 157us/step - loss: 0.5678 - acc: 0.7737 - val_loss: 1.1245 - val_acc: 0.5890\n",
      "Epoch 8/10\n",
      "124848/124848 [==============================] - 20s 160us/step - loss: 0.5500 - acc: 0.7824 - val_loss: 1.1376 - val_acc: 0.5883\n",
      "Epoch 9/10\n",
      "124848/124848 [==============================] - 19s 156us/step - loss: 0.5340 - acc: 0.7894 - val_loss: 1.1638 - val_acc: 0.5879\n",
      "Epoch 10/10\n",
      "124848/124848 [==============================] - 18s 148us/step - loss: 0.5208 - acc: 0.7962 - val_loss: 1.1645 - val_acc: 0.5848\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f18028292e8>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNN_model.fit(X_train, y, validation_split=0.2, epochs = 10, batch_size=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 124848 samples, validate on 31212 samples\n",
      "Epoch 1/10\n",
      "124848/124848 [==============================] - 292s 2ms/step - loss: 1.1270 - acc: 0.5551 - val_loss: 1.1025 - val_acc: 0.5458\n",
      "Epoch 2/10\n",
      "124848/124848 [==============================] - 299s 2ms/step - loss: 0.9756 - acc: 0.6161 - val_loss: 1.0373 - val_acc: 0.5901\n",
      "Epoch 3/10\n",
      "124848/124848 [==============================] - 297s 2ms/step - loss: 0.8330 - acc: 0.6677 - val_loss: 1.0124 - val_acc: 0.6079\n",
      "Epoch 4/10\n",
      "124848/124848 [==============================] - 295s 2ms/step - loss: 0.7753 - acc: 0.6884 - val_loss: 1.0235 - val_acc: 0.6021\n",
      "Epoch 5/10\n",
      "124848/124848 [==============================] - 292s 2ms/step - loss: 0.7407 - acc: 0.7010 - val_loss: 0.9905 - val_acc: 0.6091\n",
      "Epoch 6/10\n",
      "124848/124848 [==============================] - 292s 2ms/step - loss: 0.7121 - acc: 0.7128 - val_loss: 1.0013 - val_acc: 0.6028\n",
      "Epoch 7/10\n",
      "124848/124848 [==============================] - 296s 2ms/step - loss: 0.6910 - acc: 0.7208 - val_loss: 1.0262 - val_acc: 0.6021\n",
      "Epoch 8/10\n",
      "124848/124848 [==============================] - 295s 2ms/step - loss: 0.6732 - acc: 0.7265 - val_loss: 1.0294 - val_acc: 0.6077\n",
      "Epoch 9/10\n",
      "124848/124848 [==============================] - 293s 2ms/step - loss: 0.6575 - acc: 0.7339 - val_loss: 1.0547 - val_acc: 0.6062\n",
      "Epoch 10/10\n",
      "124848/124848 [==============================] - 296s 2ms/step - loss: 0.6446 - acc: 0.7375 - val_loss: 1.0404 - val_acc: 0.6034\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1801a1d6d8>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RNN_model.fit(X_train, y, validation_split=0.2, epochs = 10, batch_size=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_model.save('rnn_model.h5')\n",
    "CNN_model.save('cnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Zhao",
   "language": "python",
   "name": "zhao"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
